{"metadata":{"colab":{"provenance":[],"include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/Page0526/Pytorch-crash-course/blob/main/deep-neural-networks/nlp/Neural_Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"# Task\nOur task is to implement a based neural language model.\n\nTraining data: WikiText-2\n\nTest data: WikiText-103\n\nEvaluation criteria: Perplexity on the test dataset\n\nReference implementation: https://colab.research.google.com/drive/1-v8bRLr-UWQnxNA8m7Xpt4-XF8Ki_40T?usp=sharingLinks to an external site.\n\nYou are recommended to use the preprocessing function in the reference implementation to avoid distribution shifts.\n\nSubmission format: you have to submit the Jupyter Notebook. The notebook should clearly show that you use the WikiText-2 for training and WikiText-103 for testing. The perplexity should be displayed in the output, I will not rerun the code to get the perplexity.","metadata":{"id":"q6xlqXIsx-Z_"}},{"cell_type":"markdown","source":"# Sample from Lecturer","metadata":{"id":"gm_zPQ772JZf"}},{"cell_type":"code","source":"# interactive features of VSCode, Colab, Jupyter notebook\n!pip install ipympl","metadata":{"id":"L5kDkntz7_zJ","outputId":"2b62b358-ae49-4be8-da01-86bed668222f","execution":{"iopub.status.busy":"2024-10-10T00:35:28.865922Z","iopub.execute_input":"2024-10-10T00:35:28.866798Z","iopub.status.idle":"2024-10-10T00:35:41.994165Z","shell.execute_reply.started":"2024-10-10T00:35:28.866754Z","shell.execute_reply":"2024-10-10T00:35:41.993148Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Requirement already satisfied: ipympl in /opt/conda/lib/python3.10/site-packages (0.7.0)\nRequirement already satisfied: ipykernel>=4.7 in /opt/conda/lib/python3.10/site-packages (from ipympl) (6.29.4)\nRequirement already satisfied: ipywidgets>=7.6.0 in /opt/conda/lib/python3.10/site-packages (from ipympl) (7.7.1)\nRequirement already satisfied: matplotlib>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from ipympl) (3.7.5)\nRequirement already satisfied: comm>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.7->ipympl) (0.2.2)\nRequirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.7->ipympl) (1.8.1)\nRequirement already satisfied: ipython>=7.23.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.7->ipympl) (8.21.0)\nRequirement already satisfied: jupyter-client>=6.1.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.7->ipympl) (7.4.9)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.7->ipympl) (5.7.2)\nRequirement already satisfied: matplotlib-inline>=0.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.7->ipympl) (0.1.7)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.7->ipympl) (1.6.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.7->ipympl) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.7->ipympl) (5.9.3)\nRequirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.7->ipympl) (26.0.3)\nRequirement already satisfied: tornado>=6.1 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.7->ipympl) (6.4.1)\nRequirement already satisfied: traitlets>=5.4.0 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=4.7->ipympl) (5.14.3)\nRequirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>=7.6.0->ipympl) (0.2.0)\nRequirement already satisfied: widgetsnbextension~=3.6.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>=7.6.0->ipympl) (3.6.9)\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets>=7.6.0->ipympl) (3.0.11)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0.0->ipympl) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0.0->ipympl) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0.0->ipympl) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0.0->ipympl) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0.0->ipympl) (1.26.4)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0.0->ipympl) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0.0->ipympl) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=2.0.0->ipympl) (2.9.0.post0)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=4.7->ipympl) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=4.7->ipympl) (0.19.1)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=4.7->ipympl) (3.0.47)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=4.7->ipympl) (2.18.0)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=4.7->ipympl) (0.6.2)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=4.7->ipympl) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.23.1->ipykernel>=4.7->ipympl) (4.9.0)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from jupyter-client>=6.1.12->ipykernel>=4.7->ipympl) (0.4)\nRequirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel>=4.7->ipympl) (3.11.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->ipympl) (1.16.0)\nRequirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.10/site-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (6.5.7)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=4.7->ipympl) (0.8.4)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (3.1.4)\nRequirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (23.1.0)\nRequirement already satisfied: nbformat in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (5.10.4)\nRequirement already satisfied: nbconvert>=5 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (6.4.5)\nRequirement already satisfied: Send2Trash>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (1.8.3)\nRequirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (0.18.1)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (0.20.0)\nRequirement already satisfied: nbclassic>=0.4.7 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (1.1.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=4.7->ipympl) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel>=4.7->ipympl) (0.2.13)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=4.7->ipympl) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=4.7->ipympl) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.23.1->ipykernel>=4.7->ipympl) (0.2.2)\nRequirement already satisfied: notebook-shim>=0.2.3 in /opt/conda/lib/python3.10/site-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (0.2.4)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (0.8.4)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (0.3.0)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (6.1.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (1.5.0)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (0.6.0)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (0.7.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (4.12.3)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (0.5.13)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (2.1.5)\nRequirement already satisfied: fastjsonschema>=2.15 in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (2.19.1)\nRequirement already satisfied: jsonschema>=2.6 in /opt/conda/lib/python3.10/site-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (4.22.0)\nRequirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (21.2.0)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (0.35.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (0.18.1)\nRequirement already satisfied: jupyter-server<3,>=1.8 in /opt/conda/lib/python3.10/site-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (2.12.5)\nRequirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (1.16.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (2.5)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (0.5.1)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (2.22)\nRequirement already satisfied: anyio>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (4.4.0)\nRequirement already satisfied: jupyter-events>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (0.10.0)\nRequirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (0.5.3)\nRequirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (7.7.0)\nRequirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (1.8.0)\nRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (3.7)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (1.3.1)\nRequirement already satisfied: typing-extensions>=4.1 in /opt/conda/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (4.12.2)\nRequirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (2.0.7)\nRequirement already satisfied: pyyaml>=5.3 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (6.0.2)\nRequirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (0.1.4)\nRequirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (0.1.1)\nRequirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (1.5.1)\nRequirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (2.4)\nRequirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (1.3.0)\nRequirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (24.6.0)\nRequirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (1.3.0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->ipympl) (2.9.0.20240316)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\n# %matplotlib widget\n# from google.colab import output\n# output.enable_custom_widget_manager()","metadata":{"id":"IJcyCjz9QLja","execution":{"iopub.status.busy":"2024-10-07T07:32:08.436316Z","iopub.execute_input":"2024-10-07T07:32:08.436715Z","iopub.status.idle":"2024-10-07T07:32:12.709860Z","shell.execute_reply.started":"2024-10-07T07:32:08.436665Z","shell.execute_reply":"2024-10-07T07:32:12.708871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = \"\"\"A closed-loop controller or feedback controller is a control loop which incorporates feedback, in contrast to an open-loop controller or non-feedback controller. A closed-loop controller uses feedback to control states or outputs of a dynamical system. Its name comes from the information path in the system: process inputs (e.g., voltage applied to an electric motor) have an effect on the process outputs (e.g., speed or torque of the motor), which is measured with sensors and processed by the controller; the result (the control signal) is \"fed back\" as input to the process, closing the loop\"\"\"\ntest_data = \"TBD\"","metadata":{"id":"MC4CvZpbQ0_N","execution":{"iopub.status.busy":"2024-10-07T07:32:17.677313Z","iopub.execute_input":"2024-10-07T07:32:17.678051Z","iopub.status.idle":"2024-10-07T07:32:17.682739Z","shell.execute_reply.started":"2024-10-07T07:32:17.678005Z","shell.execute_reply":"2024-10-07T07:32:17.681767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tqdm\nimport re\n\n'''\nPreprocessing process\n1. Lower all characters\n2. Remove all characters that are not alpha-bet or number\n3. Create vocab dictionary\n4. Create token for words\n'''\ndef processing(text: str):\n    text = text.lower()\n    corpus = re.sub(r'[^a-z\\s]', '', text).split()\n    vocab = sorted(set(corpus))\n    vocab.append('<pad>')\n    vocab.append('<unk>')\n    word_2_id = {w: i for i, w in enumerate(vocab)}\n    id_2_word = {i: w for i, w in enumerate(vocab)}\n    text_id = [word_2_id[w] for w in corpus]\n\n    return corpus, vocab, text_id, word_2_id, id_2_word\n\ncorpus, vocab, text_id, word_2_id, id_2_word = processing(train_data)\n\n'''\nDefine a Neural Language Model\n1. V = len of the vocab dictionary\n2. D = dimension\n3. N = context window\n'''\nV = len(vocab)\nD = 100\nN = 5\nn_hidden = 512\nnet = nn.Sequential(\n    nn.Embedding(num_embeddings=V, embedding_dim=D),\n    nn.LeakyReLU(negative_slope=0.15),\n    nn.Flatten(),\n    nn.Linear(in_features=D*N, out_features=n_hidden),\n    nn.LeakyReLU(negative_slope=0.15),\n    nn.Linear(in_features=n_hidden, out_features=n_hidden),\n    nn.LeakyReLU(negative_slope=0.15),\n    nn.Linear(in_features=n_hidden, out_features=V)\n)\n\n'''\nLoss fn = CrossEntropyLoss\nOptimizer = Adam/SGD\nEPOCHS = 100\n'''\ndef train(text_id: list, net: nn.Module, lr: float, optimizer: str, nepochs: int, N: int):\n    fig,ax = plt.subplots(1,1)\n    losses = []\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(net.parameters(), lr=lr) if optimizer == \"adam\" else optim.SGD(net.parameters(), lr=lr)\n    for ei in tqdm.trange(nepochs):\n        for i in range(len(text_id) - N):\n            inputs = torch.LongTensor(text_id[i:i+N]).reshape(1, -1)\n            target = torch.LongTensor([text_id[i+N]])\n            # inputs.shape -> [1, 5]\n            output = net(inputs)\n            loss = criterion(output, target)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        \n        losses.append(loss.item())\n    ax.clear()\n    ax.plot(losses)\n    fig.canvas.draw()\n    fig.canvas.flush_events()\n    # test()    # perplexity\n    return net\n\n\ndef test(text: str, net: nn.Module):\n    pass\n\n\ndef text2id(text):\n    corpus = text.lower()\n    corpus = re.sub(r'[^a-z\\s]', '', corpus).split()\n    text_id = [word_2_id.get(w, V - 1) for w in corpus]\n    return text_id\n\n\ndef predict(net: nn.Module, text: str, N: int):\n    text_id = text2id(text)\n    if len(text_id) < N:\n        text_id = [word_2_id['<pad>']] * (N - len(text_id)) + text_id\n        text_id = text_id[-N:]\n    # print(f\"{text_id=}\")\n    prob = net(torch.LongTensor(text_id).reshape(1, -1))\n    prob = torch.softmax(prob, dim=1)\n    # print(f\"{prob=}\")\n    next_word_id = torch.argmax(prob).item()\n    # print(f\"{next_word_id=}\")\n    return id_2_word[next_word_id], prob.detach().numpy()[0]\n\ndef perplexity(net: nn.Module, text: str):\n    pass","metadata":{"id":"Rv6_csjVQRIk","execution":{"iopub.status.busy":"2024-10-07T07:33:02.924528Z","iopub.execute_input":"2024-10-07T07:33:02.925256Z","iopub.status.idle":"2024-10-07T07:33:02.949925Z","shell.execute_reply.started":"2024-10-07T07:33:02.925214Z","shell.execute_reply":"2024-10-07T07:33:02.949018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(text_id), V","metadata":{"execution":{"iopub.status.busy":"2024-10-07T08:35:14.965420Z","iopub.execute_input":"2024-10-07T08:35:14.965928Z","iopub.status.idle":"2024-10-07T08:35:14.972132Z","shell.execute_reply.started":"2024-10-07T08:35:14.965888Z","shell.execute_reply":"2024-10-07T08:35:14.971049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(text_id, net, lr=1e-3, optimizer='adam', nepochs=100, N=5)","metadata":{"id":"kvVUZehF7OKf","outputId":"2d41b087-7832-4341-db5c-8be8f7b2a164","execution":{"iopub.status.busy":"2024-10-07T07:33:05.956031Z","iopub.execute_input":"2024-10-07T07:33:05.956953Z","iopub.status.idle":"2024-10-07T07:33:30.276297Z","shell.execute_reply.started":"2024-10-07T07:33:05.956910Z","shell.execute_reply":"2024-10-07T07:33:30.275286Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Support for third party widgets will remain active for the duration of the session. To disable support:","metadata":{"id":"ToxnDkLx8mFK"}},{"cell_type":"code","source":"next_word, prob = predict(net, \"closedloop\", N=5)\nprint(f\"{next_word=}\")\nprint()\nprint(\"Prob\")\nprint(\"\\n\".join([f\"{id_2_word[i]} {prob[i]}\" for i in range(len(prob))]))\nplt.figure(figsize=(10, 5))\nplt.bar(range(len(prob)), prob)\nplt.xticks(range(len(prob)), id_2_word.values(), rotation=90)\nplt.show()","metadata":{"id":"XuyldAPSAnNe","outputId":"f1ccbf47-f554-4a81-d3f9-3035f9eba2b4","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_perplexity = perplexity(net, test_data)\nprint(f\"Test perplexity: {test_perplexity}\")","metadata":{"id":"4UZcDwgXubgw","outputId":"c51ab656-e2c1-4ec2-afd2-35a84bc9915f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# My solution","metadata":{"id":"wociin9b2NJE"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch.utils.data import DataLoader, Dataset\nfrom datasets import load_dataset","metadata":{"id":"9kF91edn2N9j","execution":{"iopub.status.busy":"2024-10-10T00:35:41.996050Z","iopub.execute_input":"2024-10-10T00:35:41.996368Z","iopub.status.idle":"2024-10-10T00:35:42.002254Z","shell.execute_reply.started":"2024-10-10T00:35:41.996335Z","shell.execute_reply":"2024-10-10T00:35:42.001306Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_data = load_dataset('Salesforce/wikitext','wikitext-2-v1')\ntest_data = load_dataset('Salesforce/wikitext','wikitext-103-v1')","metadata":{"id":"MVz6cqmQ2SsO","execution":{"iopub.status.busy":"2024-10-10T00:35:43.119477Z","iopub.execute_input":"2024-10-10T00:35:43.120223Z","iopub.status.idle":"2024-10-10T00:35:46.459209Z","shell.execute_reply.started":"2024-10-10T00:35:43.120182Z","shell.execute_reply":"2024-10-10T00:35:46.458429Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_data, test_data","metadata":{"execution":{"iopub.status.busy":"2024-10-10T00:35:54.636972Z","iopub.execute_input":"2024-10-10T00:35:54.637600Z","iopub.status.idle":"2024-10-10T00:35:54.643772Z","shell.execute_reply.started":"2024-10-10T00:35:54.637559Z","shell.execute_reply":"2024-10-10T00:35:54.642915Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(DatasetDict({\n     test: Dataset({\n         features: ['text'],\n         num_rows: 4358\n     })\n     train: Dataset({\n         features: ['text'],\n         num_rows: 36718\n     })\n     validation: Dataset({\n         features: ['text'],\n         num_rows: 3760\n     })\n }),\n DatasetDict({\n     test: Dataset({\n         features: ['text'],\n         num_rows: 4358\n     })\n     train: Dataset({\n         features: ['text'],\n         num_rows: 1801350\n     })\n     validation: Dataset({\n         features: ['text'],\n         num_rows: 3760\n     })\n }))"},"metadata":{}}]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2024-10-10T00:35:52.324041Z","iopub.execute_input":"2024-10-10T00:35:52.324416Z","iopub.status.idle":"2024-10-10T00:35:53.740843Z","shell.execute_reply.started":"2024-10-10T00:35:52.324381Z","shell.execute_reply":"2024-10-10T00:35:53.739919Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom collections import Counter\n\nss = SnowballStemmer('english')\nsw = stopwords.words('english')","metadata":{"execution":{"iopub.status.busy":"2024-10-10T00:35:57.447926Z","iopub.execute_input":"2024-10-10T00:35:57.448319Z","iopub.status.idle":"2024-10-10T00:35:57.455530Z","shell.execute_reply.started":"2024-10-10T00:35:57.448279Z","shell.execute_reply":"2024-10-10T00:35:57.454714Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"test_data['test'][0]","metadata":{"execution":{"iopub.status.busy":"2024-10-09T10:29:52.401513Z","iopub.execute_input":"2024-10-09T10:29:52.402445Z","iopub.status.idle":"2024-10-09T10:29:52.412540Z","shell.execute_reply.started":"2024-10-09T10:29:52.402380Z","shell.execute_reply":"2024-10-09T10:29:52.411214Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'text': ''}"},"metadata":{}}]},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\nclass NeuralNetworkDataset(Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        self.dataset = self.dataset.map(self.preprocess)\n        self.create_vocab()\n        self.dataset = self.dataset.map(self.remove_rare_tokens)\n        self.dataset = self.dataset.map(self.text2id)\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        row = self.dataset[idx]\n        text_ids = row['text_id']\n        return text_ids\n    \n    def preprocess(self, row):\n        row['all_tokens'] = [ss.stem(i) for i in \n                             re.split(r\" +\",\n                                  re.sub(r\"[^a-z@# ]\",\"\",\n                                         row['text'].lower()))\n                            if (i not in sw) and len(i)]\n        return row\n    \n    def create_vocab(self):\n        counts = Counter([i for s in self.dataset['all_tokens'] for i in s])\n        counts = {k:v for k, v in counts.items() if v>10} # filtering\n        self.vocab = list(counts.keys())\n        self.vocab.append('<pad>')\n        self.vocab.append('<unk>')\n        self.n_v = len(self.vocab)\n        self.id2tok = dict(enumerate(self.vocab))\n        self.tok2id = {token: id for id, token in self.id2tok.items()}\n\n    def remove_rare_tokens(self, row):\n        row['tokens'] = [t for t in row['all_tokens'] if t in self.vocab]\n        return row\n    \n    def text2id(self, row):\n        row['text_id'] = [self.tok2id.get(w, self.tok2id['<unk>']) for s in row['all_tokens'] for w in s]\n        return row\n    \n    def collate_fn(self, batch):\n        return pad_sequence([torch.tensor(item) for item in batch], batch_first=True, padding_value=self.tok2id['<pad>'])","metadata":{"execution":{"iopub.status.busy":"2024-10-10T00:36:00.121208Z","iopub.execute_input":"2024-10-10T00:36:00.121596Z","iopub.status.idle":"2024-10-10T00:36:00.135402Z","shell.execute_reply.started":"2024-10-10T00:36:00.121557Z","shell.execute_reply":"2024-10-10T00:36:00.134545Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"trainset = NeuralNetworkDataset(train_data['train'])","metadata":{"execution":{"iopub.status.busy":"2024-10-10T00:36:01.364597Z","iopub.execute_input":"2024-10-10T00:36:01.365074Z","iopub.status.idle":"2024-10-10T00:36:23.652874Z","shell.execute_reply.started":"2024-10-10T00:36:01.365014Z","shell.execute_reply":"2024-10-10T00:36:23.651191Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eec16f7238f2451d84f70ae9bc5d9931"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainset \u001b[38;5;241m=\u001b[39m \u001b[43mNeuralNetworkDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36mNeuralNetworkDataset.__init__\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m dataset\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_vocab()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremove_rare_tokens)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3035\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3030\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3031\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3032\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3033\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3034\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3035\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3036\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3037\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3408\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3406\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3407\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3408\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3410\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:3300\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3299\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3300\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3302\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3303\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3304\u001b[0m     }\n","Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36mNeuralNetworkDataset.preprocess\u001b[0;34m(self, row)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, row):\n\u001b[0;32m---> 20\u001b[0m     row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [ss\u001b[38;5;241m.\u001b[39mstem(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \n\u001b[1;32m     21\u001b[0m                          re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m +\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m                               re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^a-z@# ]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m                                      row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()))\n\u001b[1;32m     24\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sw) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(i)]\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row\n","Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, row):\n\u001b[0;32m---> 20\u001b[0m     row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_tokens\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[43mss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \n\u001b[1;32m     21\u001b[0m                          re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m +\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m                               re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[^a-z@# ]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m                                      row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlower()))\n\u001b[1;32m     24\u001b[0m                         \u001b[38;5;28;01mif\u001b[39;00m (i \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m sw) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(i)]\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m row\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/nltk/stem/snowball.py:844\u001b[0m, in \u001b[0;36mEnglishStemmer.stem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;66;03m# STEP 2\u001b[39;00m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m suffix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step2_suffixes:\n\u001b[0;32m--> 844\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendswith\u001b[49m\u001b[43m(\u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    845\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m r1\u001b[38;5;241m.\u001b[39mendswith(suffix):\n\u001b[1;32m    846\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtional\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"testset = NeuralNetworkDataset(test_data['test'])","metadata":{"execution":{"iopub.status.busy":"2024-10-10T00:36:23.653568Z","iopub.status.idle":"2024-10-10T00:36:23.653973Z","shell.execute_reply.started":"2024-10-10T00:36:23.653766Z","shell.execute_reply":"2024-10-10T00:36:23.653786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(testset), len(trainset)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T01:58:29.599740Z","iopub.execute_input":"2024-10-09T01:58:29.600289Z","iopub.status.idle":"2024-10-09T01:58:29.610320Z","shell.execute_reply.started":"2024-10-09T01:58:29.600228Z","shell.execute_reply":"2024-10-09T01:58:29.608619Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(4358, 36718)"},"metadata":{}}]},{"cell_type":"code","source":"trainloader = DataLoader(trainset, batch_size=64, shuffle=True, collate_fn=trainset.collate_fn)\ntestloader = DataLoader(testset, batch_size=64, shuffle=False, collate_fn=testset.collate_fn)\n# test\nnext(iter(trainloader)), next(iter(testloader))","metadata":{"execution":{"iopub.status.busy":"2024-10-10T00:36:23.655178Z","iopub.status.idle":"2024-10-10T00:36:23.655509Z","shell.execute_reply.started":"2024-10-10T00:36:23.655341Z","shell.execute_reply":"2024-10-10T00:36:23.655359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"testset.dataset['text'][3]","metadata":{"execution":{"iopub.status.busy":"2024-10-09T01:58:29.754226Z","iopub.execute_input":"2024-10-09T01:58:29.754631Z","iopub.status.idle":"2024-10-09T01:58:29.770758Z","shell.execute_reply.started":"2024-10-09T01:58:29.754589Z","shell.execute_reply":"2024-10-09T01:58:29.769537Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"' Robert Boulter is an English film , television and theatre actor . He had a guest @-@ starring role on the television series The Bill in 2000 . This was followed by a starring role in the play Herons written by Simon Stephens , which was performed in 2001 at the Royal Court Theatre . He had a guest role in the television series Judge John Deed in 2002 . In 2004 Boulter landed a role as \" Craig \" in the episode \" Teddy \\'s Story \" of the television series The Long Firm ; he starred alongside actors Mark Strong and Derek Jacobi . He was cast in the 2005 theatre productions of the Philip Ridley play Mercury Fur , which was performed at the Drum Theatre in Plymouth and the <unk> Chocolate Factory in London . He was directed by John Tiffany and starred alongside Ben Whishaw , Shane Zaza , Harry Kent , Fraser Ayres , Sophie Stanton and Dominic Hall . \\n'"},"metadata":{}}]},{"cell_type":"code","source":"testset.dataset['all_tokens'][3]","metadata":{"execution":{"iopub.status.busy":"2024-10-09T01:58:29.772326Z","iopub.execute_input":"2024-10-09T01:58:29.772742Z","iopub.status.idle":"2024-10-09T01:58:29.929204Z","shell.execute_reply.started":"2024-10-09T01:58:29.772701Z","shell.execute_reply":"2024-10-09T01:58:29.927830Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"['robert',\n 'boulter',\n 'english',\n 'film',\n 'televis',\n 'theatr',\n 'actor',\n 'guest',\n '@@',\n 'star',\n 'role',\n 'televis',\n 'seri',\n 'bill',\n 'follow',\n 'star',\n 'role',\n 'play',\n 'heron',\n 'written',\n 'simon',\n 'stephen',\n 'perform',\n 'royal',\n 'court',\n 'theatr',\n 'guest',\n 'role',\n 'televis',\n 'seri',\n 'judg',\n 'john',\n 'deed',\n 'boulter',\n 'land',\n 'role',\n 'craig',\n 'episod',\n 'teddi',\n 'stori',\n 'televis',\n 'seri',\n 'long',\n 'firm',\n 'star',\n 'alongsid',\n 'actor',\n 'mark',\n 'strong',\n 'derek',\n 'jacobi',\n 'cast',\n 'theatr',\n 'product',\n 'philip',\n 'ridley',\n 'play',\n 'mercuri',\n 'fur',\n 'perform',\n 'drum',\n 'theatr',\n 'plymouth',\n 'unk',\n 'chocol',\n 'factori',\n 'london',\n 'direct',\n 'john',\n 'tiffani',\n 'star',\n 'alongsid',\n 'ben',\n 'whishaw',\n 'shane',\n 'zaza',\n 'harri',\n 'kent',\n 'fraser',\n 'ayr',\n 'sophi',\n 'stanton',\n 'domin',\n 'hall']"},"metadata":{}}]},{"cell_type":"code","source":"testset[3]","metadata":{"execution":{"iopub.status.busy":"2024-10-09T01:58:29.930896Z","iopub.execute_input":"2024-10-09T01:58:29.931406Z","iopub.status.idle":"2024-10-09T01:58:29.946597Z","shell.execute_reply.started":"2024-10-09T01:58:29.931352Z","shell.execute_reply":"2024-10-09T01:58:29.945262Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"[1235,\n 2160,\n 1401,\n 749,\n 1235,\n 2160,\n 1401,\n 2160,\n 1431,\n 1426,\n 2160,\n 749,\n 1235,\n 749,\n 1929,\n 2160,\n 1426,\n 2160,\n 2160,\n 936,\n 1436,\n 2160,\n 1426,\n 2160,\n 2160,\n 749,\n 1426,\n 749,\n 1861,\n 2160,\n 2160,\n 2160,\n 936,\n 749,\n 2160,\n 2160,\n 1235,\n 2160,\n 750,\n 2160,\n 2160,\n 1235,\n 2160,\n 1431,\n 749,\n 2160,\n 2160,\n 2160,\n 2160,\n 2160,\n 2160,\n 2160,\n 1235,\n 1235,\n 2160,\n 1426,\n 749,\n 2160,\n 749,\n 1426,\n 749,\n 1861,\n 2160,\n 2160,\n 2160,\n 749,\n 1235,\n 2160,\n 1401,\n 2160,\n 1426,\n 1426,\n 1436,\n 2160,\n 1426,\n 1426,\n 2160,\n 1477,\n 2160,\n 2160,\n 2160,\n 1235,\n 1235,\n 2160,\n 1426,\n 749,\n 637,\n 1426,\n 2160,\n 2160,\n 936,\n 749,\n 1235,\n 2160,\n 1929,\n 1477,\n 1235,\n 2160,\n 2160,\n 2160,\n 749,\n 1929,\n 2160,\n 2160,\n 2160,\n 2160,\n 1929,\n 2160,\n 2160,\n 749,\n 637,\n 936,\n 749,\n 1929,\n 637,\n 749,\n 1235,\n 1436,\n 2160,\n 1235,\n 2160,\n 1235,\n 2160,\n 2160,\n 2160,\n 1426,\n 750,\n 2160,\n 1431,\n 1235,\n 2160,\n 2160,\n 936,\n 749,\n 2160,\n 2160,\n 1235,\n 2160,\n 1431,\n 749,\n 2160,\n 2160,\n 1235,\n 2160,\n 1426,\n 749,\n 2160,\n 749,\n 1426,\n 749,\n 1861,\n 2160,\n 2160,\n 2160,\n 749,\n 1235,\n 2160,\n 2160,\n 1431,\n 2160,\n 2160,\n 2160,\n 2160,\n 936,\n 1929,\n 2160,\n 749,\n 749,\n 2160,\n 1401,\n 2160,\n 1431,\n 1426,\n 2160,\n 749,\n 1235,\n 1426,\n 2160,\n 1929,\n 2160,\n 1235,\n 2160,\n 1426,\n 749,\n 750,\n 1235,\n 2160,\n 2160,\n 2160,\n 749,\n 637,\n 2160,\n 2160,\n 2160,\n 2160,\n 2160,\n 749,\n 2160,\n 2160,\n 2160,\n 2160,\n 2160,\n 2160,\n 1235,\n 2160,\n 2160,\n 749,\n 1426,\n 749,\n 1861,\n 2160,\n 2160,\n 2160,\n 749,\n 1235,\n 2160,\n 1426,\n 2160,\n 1929,\n 2160,\n 1436,\n 2160,\n 1235,\n 2160,\n 2160,\n 2160,\n 2160,\n 1235,\n 2160,\n 1426,\n 2160,\n 1929,\n 2160,\n 2160,\n 2160,\n 2160,\n 2160,\n 750,\n 2160,\n 2160,\n 1235,\n 2160,\n 2160,\n 1235,\n 1437,\n 2160,\n 2160,\n 1235,\n 2160,\n 1929,\n 2160,\n 2160,\n 749,\n 1235,\n 749,\n 1437,\n 2160,\n 2160,\n 750,\n 2160,\n 1401,\n 2160,\n 750,\n 2160,\n 2160,\n 2160,\n 2160,\n 936,\n 749,\n 2160,\n 2160,\n 1235,\n 637,\n 1235,\n 2160,\n 2160,\n 1431,\n 750,\n 2160,\n 637,\n 936,\n 2160,\n 1426,\n 2160,\n 637,\n 1235,\n 2160,\n 2160,\n 1426,\n 749,\n 2160,\n 637,\n 1426,\n 2160,\n 2160,\n 2160,\n 749,\n 1235,\n 750,\n 1431,\n 1235,\n 2160,\n 1436,\n 1431,\n 1235,\n 637,\n 749,\n 1235,\n 1436,\n 2160,\n 1235,\n 2160,\n 2160,\n 1235,\n 1431,\n 2160,\n 2160,\n 936,\n 749,\n 2160,\n 2160,\n 1235,\n 637,\n 1426,\n 2160,\n 2160,\n 2160,\n 1431,\n 2160,\n 936,\n 1431,\n 1929,\n 1437,\n 750,\n 936,\n 2160,\n 750,\n 2160,\n 1426,\n 1436,\n 2160,\n 750,\n 2160,\n 2160,\n 1235,\n 2160,\n 1426,\n 2160,\n 1929,\n 2160,\n 2160,\n 1929,\n 2160,\n 2160,\n 1235,\n 749,\n 750,\n 2160,\n 2160,\n 2160,\n 936,\n 1929,\n 2160,\n 2160,\n 1436,\n 1436,\n 2160,\n 1929,\n 2160,\n 2160,\n 2160,\n 2160,\n 1235,\n 2160,\n 1426,\n 2160,\n 1929,\n 2160,\n 2160,\n 2160,\n 2160,\n 1401,\n 749,\n 1929,\n 1477,\n 936,\n 2160,\n 2160,\n 936,\n 2160,\n 1477,\n 2160,\n 936,\n 2160,\n 1929,\n 749,\n 2160,\n 2160,\n 2160,\n 2160,\n 936,\n 2160,\n 1235,\n 1235,\n 2160,\n 1437,\n 749,\n 1929,\n 2160,\n 1436,\n 1235,\n 2160,\n 2160,\n 749,\n 1235,\n 2160,\n 2160,\n 1235,\n 2160,\n 2160,\n 637,\n 936,\n 2160,\n 2160,\n 2160,\n 2160,\n 1929,\n 2160,\n 2160,\n 1929,\n 2160,\n 2160,\n 2160,\n 2160,\n 1929,\n 936,\n 2160,\n 1426,\n 1426]"},"metadata":{}}]},{"cell_type":"code","source":"class NeuralLanguageModel(nn.Module):\n    def __init__(self, v_size, dim, w_size, n_hidden):\n        super(NeuralLanguageModel, self).__init__()\n        self.model = nn.Sequential(\n            nn.Embedding(num_embeddings=v_size, embedding_dim=dim),\n            nn.LeakyReLU(negative_slope=0.15),\n            nn.Flatten(),\n            nn.Linear(in_features=dim*w_size, out_features=n_hidden),\n            nn.LeakyReLU(negative_slope=0.15),\n            nn.Linear(in_features=n_hidden, out_features=n_hidden),\n            nn.LeakyReLU(negative_slope=0.15),\n            nn.Linear(in_features=n_hidden, out_features=v_size)\n        )\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T00:36:23.656929Z","iopub.status.idle":"2024-10-10T00:36:23.657262Z","shell.execute_reply.started":"2024-10-10T00:36:23.657095Z","shell.execute_reply":"2024-10-10T00:36:23.657112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab=trainset.vocab\nv_size=len(vocab)\n\nneural_model = NeuralLanguageModel(v_size=v_size, dim=100, w_size=5, n_hidden=64)\nneural_model","metadata":{"execution":{"iopub.status.busy":"2024-10-10T00:36:23.658237Z","iopub.status.idle":"2024-10-10T00:36:23.658569Z","shell.execute_reply.started":"2024-10-10T00:36:23.658395Z","shell.execute_reply":"2024-10-10T00:36:23.658413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"config =  {\n        \"shuffle\": True,\n        \"lr\": 1e-3,\n        \"epochs\": 3,\n        \"train_steps\":1, \n        \"val_steps\":1, \n        \"checkpoint_frequency\": 1\n        }\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(neural_model.parameters(), lr = config['lr'])\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install lightning","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import lightning as L\nimport os\nfrom tqdm import tqdm\n\nclass Trainer:\n    def __init__(self, model, criterion, vocab, dataloader, optimizer, device, config, w_size):\n        self.model = model.to(device)\n        self.criterion = criterion\n        self.vocab = vocab\n        self.dataloader = dataloader\n        self.optimizer = optimizer\n        self.device = device\n        self.w_size = w_size\n        self.config = config\n        self.loss = {\n            \"train\": [],\n            \"val\": []\n        }\n        \n    def train(self):\n        \n        for epoch in tqdm(range(self.config['epochs'])):\n                self.train_step()\n                self.val_step()\n                \n                print(f\"Epoch: {epoch + 1}/{self.config['epochs']}, Train Loss {self.loss['train'][-1]:.5f}, Val Loss {self.loss['val'][-1]:.5f}\")\n\n    def train_step(self):\n        running_loss = []\n        for idx, sentence in enumerate(self.dataloader):\n            if sentence.shape[1] == 0:\n                continue\n            print(f\"index {idx}| sentence.shape {sentence.shape}\")\n            for i in range(sentence.shape[1] - self.w_size):\n                inputs = sentence[0][i:i+self.w_size].clone().detach().reshape(1, -1)\n                target = sentence[0][i+self.w_size].clone().detach()\n                print(f\"i {i}|target {target.shape}\")\n                output = self.model(inputs)\n                loss = self.criterion(output, target)\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n                running_loss.append([loss.item()])\n        self.loss['train'].append(np.mean(running_loss))\n    \n    def val_step(self):\n        val_loss = []\n        self.model.eval()\n        \n        with torch.inference_mode():\n            for idx, sentence in enumerate(self.dataloader, 1):\n                if sentence.shape[1] == 0:\n                    continue\n                for i in range(sentence.shape[1] - self.w_size):\n                    inputs = torch.tensor(sentence[0][i:i+self.w_size], dtype=torch.long, device=device).reshape(1, -1)\n                    target = torch.tensor(sentence[0][i+self.w_size], dtype=torch.long, device=device)\n                    output = self.model(inputs)\n                    loss = self.criterion(output, target)\n\n                    val_loss.append(loss.item())\n                \n        self.loss['val'].append(np.mean(val_loss))\n        \n    def save_model(self, save_path):\n        model_path = os.path.join(save_dir, \"model.pt\")\n        torch.save(self.model, model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(trainloader)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T14:27:31.513422Z","iopub.execute_input":"2024-10-09T14:27:31.514390Z","iopub.status.idle":"2024-10-09T14:27:31.520290Z","shell.execute_reply.started":"2024-10-09T14:27:31.514344Z","shell.execute_reply":"2024-10-09T14:27:31.519366Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"574"},"metadata":{}}]},{"cell_type":"code","source":"trainer = Trainer(neural_model, criterion, vocab, trainloader, optimizer, device, config, 5)\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T14:32:54.266125Z","iopub.execute_input":"2024-10-09T14:32:54.266815Z","iopub.status.idle":"2024-10-09T14:32:54.552461Z","shell.execute_reply.started":"2024-10-09T14:32:54.266766Z","shell.execute_reply":"2024-10-09T14:32:54.551165Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stderr","text":"  0%|          | 0/3 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"index 0| sentence.shape torch.Size([64, 558])\ni 0|target torch.Size([])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(neural_model, criterion, vocab, trainloader, optimizer, device, config, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[43], line 23\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[0;32m---> 23\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_step()\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[43], line 38\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m target \u001b[38;5;241m=\u001b[39m sentence[\u001b[38;5;241m0\u001b[39m][i\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_size]\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m|target \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(output, target)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[10], line 16\u001b[0m, in \u001b[0;36mNeuralLanguageModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/sparse.py:164\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:2267\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2264\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)"],"ename":"RuntimeError","evalue":"Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.FloatTensor instead (while checking arguments for embedding)","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}